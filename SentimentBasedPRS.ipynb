{"cells":[{"cell_type":"markdown","metadata":{"id":"oeh4HcYPqpw6"},"source":["# **PROBLEM STATEMENT**"]},{"cell_type":"markdown","metadata":{"id":"L8jrIlhcq_Is"},"source":["E-commerce company Ebuss wants to grow quickly in the market to become a major leader, it has to compete with the likes of Amazon, Flipkart, etc., which are already market leaders.\n","Ebuss wants to build a model that will improve the recommendations given to the users given their past reviews and ratings.\n","\n","In order to do this, below tasks are planned to build a sentiment-based product recommendation system: \n","\n","1. Data sourcing and sentiment analysis\n","2. Building a recommendation system\n","3. Improving the recommendations using the sentiment analysis model\n","4. Deploying the end-to-end project with a user interface"]},{"cell_type":"markdown","metadata":{"id":"kPn5T4wE-3yq"},"source":["This notebook has following major sections: \n","\n","1. Exploratory data analysis\n","2. Data preprocessing\n","3. Feature extraction\n","4. Training a text classification model\n","5. Building a recommendation system\n","6. Improving the recommendations using the sentiment analysis model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PTVkk1SQxdau"},"source":["# **1. Exploratory Data Analysis**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1639375271475,"user":{"displayName":"Meenal Kumari","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07913556611253448693"},"user_tz":-330},"id":"j1mbgQRWSC6s"},"outputs":[],"source":[" #Importing libraries\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Op6jPt3k---U"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-8c1d5ac4c47a\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     _message.blocking_request(\n\u001b[0;32m--\u003e 136\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--\u003e 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["## Mounting google drive for running the notebook on Google Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1-Rx79thiewG"},"outputs":[],"source":["cd gdrive/My Drive/Colab Notebooks/Capstone project/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hTrrU6uE9fFK"},"outputs":[],"source":["#Importing labeled data for training the classifier\n","master_df = pd.read_csv('sample30.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7G2KLvry93Cx"},"outputs":[],"source":["master_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"apiD3-rjp8pb"},"outputs":[],"source":["#Checking the datatype of columns\n","\n","master_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wCGRZQl594mQ"},"outputs":[],"source":["#We will drop the columns with high number of missing values, which are not so significant:\n","# reviews_date, reviews_doRecommend, reviews_userCity, reviews_userProvince\n","\n","master_df.drop(['reviews_date', 'reviews_doRecommend', 'reviews_userCity', 'reviews_userProvince'], axis=1, inplace=True)\n","master_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"isRvjKcVp5Hd"},"outputs":[],"source":["#Lets check for null/missing values in the data\n","\n","master_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"h25j6mvR-PuA"},"outputs":[],"source":["#Column 'reviews_didPurchase' has large number of missing values. \n","#Lets check the distribution of reviews over 'reviews_didPurchase'\n","\n","#Filling missing values with null\n","master_df['reviews_didPurchase'].fillna('Null', inplace=True)\n","\n","#checking distribution of reviews_didpurchased\n","plt.figure(figsize=(8,6))\n","sns.set_theme(style=\"darkgrid\")\n","ax = sns.countplot(master_df['reviews_didPurchase'],palette=\"Set3\")\n","ax.set_xlabel(xlabel=\"Customers who purchased the product\", fontsize=15)\n","ax.set_ylabel(ylabel='Count of Reviews', fontsize=15)\n","ax.axes.set_title('Reviews distributed over customers who did purchase or did not', fontsize=15)\n","ax.tick_params(labelsize=13)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"og5Y3JNLAIrM"},"outputs":[],"source":["#Lets check the count of reviews distributed over purchase\n","\n","print(\"Count of Reviews which are related to a Purchase:\")\n","master_df['reviews_didPurchase'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"StMoiokenmAl"},"source":["**Obervation:** There is very less number of reviews based on the actual purchase, and almost 50% data is missing, this column will not be of much significant. We will be dropping it later. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"faK8t1DpoTqL"},"outputs":[],"source":["#There is one row without user_sentiment label. We will drop the row later. \n","\n","master_df[master_df['user_sentiment'].isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TcvT5EhJyot7"},"outputs":[],"source":["##Looking at unique values in Key columns\n","\n","for i in ['brand', 'categories', 'manufacturer', 'name','reviews_username', 'reviews_rating', 'user_sentiment']:\n","  print(\"No. of unique %s is: %s\" %(i, master_df[i].nunique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zUfuwx-px5uY"},"outputs":[],"source":["#Checking top 10 most purchased product\n","result = master_df[master_df['reviews_didPurchase'] == True]\n","result['name'].value_counts()[0:10].plot(kind = 'barh', figsize=[15,10], fontsize=15,color='Blue').invert_yaxis()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"p4RBizcnpL8w"},"outputs":[],"source":["#Checking top 10 most trusted brands based on the positive review\n","from matplotlib import cm\n","result = master_df[(master_df.user_sentiment==\"Positive\")]\n","result['brand'].value_counts()[0:10].plot(kind = 'barh', figsize=[15,10], fontsize=15,color='Green').invert_yaxis()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yXLWGRuDyRnp"},"outputs":[],"source":["#Checking top 10 most badly rated brands based on the negative review\n","from matplotlib import cm\n","result = master_df[(master_df.user_sentiment==\"Negative\")]\n","result['brand'].value_counts()[0:10].plot(kind = 'barh', figsize=[15,10], fontsize=15,color='Red').invert_yaxis()"]},{"cell_type":"markdown","metadata":{"id":"U4kI0GEw5SsV"},"source":["**Observation:** Clorox brand has most number of positive as well as negative reviews. Looks like Clorox is the most reviewed brand, followed by Warner Home Video"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4XnEqaKS48vu"},"outputs":[],"source":["#Lets check Brand vs Rating\n","\n","plt.figure(figsize=(10,8))\n","ax = sns.countplot(y=master_df['brand'], hue=master_df['reviews_rating'], order=master_df['brand'].value_counts().iloc[:20].index)\n","ax.set_xlabel(xlabel=\"reviews_rating\", fontsize=15)\n","ax.set_ylabel(ylabel='brand', fontsize=15)\n","ax.axes.set_title('Brand vs Rating, grouped on Rating', fontsize=15)\n","ax.tick_params(labelsize=13)\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-r8dYmmT3ct4"},"source":["Observation: This plot confirms our observation that Clorox is the most reviewed brand.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SbfFwkMP0GqE"},"outputs":[],"source":["#overall ratings as per Reviews for all the products\n","sns.countplot(x = 'reviews_rating', data=master_df,palette = 'dark').set_title('Ratings Trend - Count of Reviews by Ratings', fontsize=14)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nHNApFqp1QAL"},"outputs":[],"source":["#Checking the count of ratings\n","master_df['reviews_rating'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"GWVXvosA1hjH"},"source":["**Observation:** Most reviews are highly rated (rating 5) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sxaZ6ukl1eo-"},"outputs":[],"source":["master_df['categories'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9FTHNi1c6NTG"},"outputs":[],"source":["# Distribution of Reviews by word length - helps understand the strength of sentiment\n","\n","f = plt.figure(figsize=(8,5))\n","df_reviews = master_df[['id','reviews_username','reviews_text','reviews_title','reviews_rating']]\n","df_reviews['reviewLength'] = df_reviews['reviews_text'].apply(lambda x: len(x.split()))\n","\n","reviews_word_length = df_reviews.groupby(pd.cut(df_reviews.reviewLength, np.arange(0,330,30))).count()\n","reviews_word_length = reviews_word_length.rename(columns={'reviewLength':'count'})\n","reviews_word_length = reviews_word_length.reset_index()\n","\n","reviewLengthChart = sns.barplot(x='reviewLength',y='count',data=reviews_word_length,palette = 'dark')\n","reviewLengthChart.set_title('Distribution of Reviews by Word Length', fontsize=15)\n","reviewLengthChart.set_xticklabels(reviewLengthChart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n","\n","f.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GmbrzRxE7pTe"},"outputs":[],"source":["# Distribution of Review Lengths by Ratings - Demonstrates How Length of Reviews relates to Ratings\n","# As most reviews are less than 150 words (shown in plot above), we will consider data with review length \u003c 150 only\n","\n","df_reviews = df_reviews[df_reviews['reviewLength'] \u003c 150]\n","\n","f = plt.figure(figsize=(8,5))\n","\n","# Distribution of Length of Reviews by Rating - Box Plot\n","reviewLength_vs_Rating = df_reviews[['id','reviewLength','reviews_rating']]\n","reviewLength_vs_Rating = sns.boxplot(x='reviews_rating', y='reviewLength', data=reviewLength_vs_Rating)\n","reviewLength_vs_Rating.set_title('Review Length vs Overall Rating', fontsize=15)\n","\n","f.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"2jq0arnm8-4I"},"source":["**Observation:** Low rated reviews are longer compared to high rated reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tYigIaaF9Oxh"},"outputs":[],"source":["# Distribution of number of reviews written by each user\n","\n","user_reviews_df = master_df[['reviews_username','id']]\n","user_reviews_df = user_reviews_df.groupby(['reviews_username']).count().reset_index()\n","user_reviews_df = user_reviews_df.sort_values('id',ascending = False)\n","user_reviews_df = user_reviews_df.rename(columns={'id':'review count'})\n","user_reviews_df.head()"]},{"cell_type":"markdown","metadata":{"id":"8CPNifEpy3XC"},"source":["# **2. Data Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TQ2u3PhBqhoV"},"outputs":[],"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","from nltk import FreqDist\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","wordnet_lemmatizer = WordNetLemmatizer()\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jI_kPg6EIl_O"},"outputs":[],"source":["#Checking duplicates for username and unique identity number \n","duplicates = master_df[master_df.duplicated(subset={\"reviews_username\",\"id\"})]\n","duplicates.reviews_username.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kMWvM5bvhqL2"},"outputs":[],"source":["# Lets look at the user 'byamazon customer' as it shows lot of duplicates\n","\n","master_df[master_df['reviews_username'] == 'byamazon customer']"]},{"cell_type":"markdown","metadata":{"id":"SNbnuPvgiUZd"},"source":["***Observation: *** User 'byamazon customer' has given multiple reviews of the same product. It is possible that the reviews are genuine, we will take average of the ratings given by a user per product.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1jShAOlAiRt5"},"outputs":[],"source":["# Take Average of Ratings\n","\n","master_df['avg_ratings'] = master_df.groupby(['id','reviews_username'])['reviews_rating'].transform('mean')\n","master_df['avg_ratings']= master_df['avg_ratings'].round(2)\n","master_df[['id','reviews_username','reviews_rating','avg_ratings']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BgpUGDaLjzlS"},"outputs":[],"source":["# We will delete duplicate Reviews for same product ID and User (Reviewer/Shopper)\n","# Copying final data to another dataframe, which will be used from here on \n","\n","df_final =  master_df.drop_duplicates(subset={\"reviews_username\",\"id\"},keep=\"first\")\n","df_final.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kbyw5XF8kXdk"},"outputs":[],"source":["# Lets check if duplicates for User = 'byamazon customer' got deleted\n","\n","df_final[df_final['reviews_username'] == 'byamazon customer']"]},{"cell_type":"markdown","metadata":{"id":"5V0vnv0Hk-Qc"},"source":["Duplicate reviews are removed. The user \"byamazon customer\" now has only 1 review for one product.\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XTHACu7AlQBl"},"outputs":[],"source":["#Lets see how much data we lost in removing the duplicates\n","\n","size_diff = df_final['id'].size/master_df['id'].size\n","\n","print(\"%.2f%% reduction in data post duplicate review deletion\"%((1-size_diff)*100))\n","print(\"Revised size of data = \",df_final['id'].size,\"rows \")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pFrW3Z6cmlDm"},"outputs":[],"source":["df_final.info()"]},{"cell_type":"markdown","metadata":{"id":"u4lW5w_yno9Y"},"source":["As the review title and review text both the columns contain text that will help us in the sentiment analysis, so we will combine the two columns together. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"E6E7Q1ZWpBYn"},"outputs":[],"source":["#Combining reviews_title and reviews_text and save as new column \"user_reviews\"\n","#Adding a period at the end of Review Titles in the new column \n","#Adding blank spaces, for Review Title with missing values \n","\n","df_final['reviews_title'] = df_final['reviews_title'].fillna('')\n","df_final['user_reviews'] = df_final[['reviews_title', 'reviews_text']].agg('. '.join, axis=1).str.lstrip('. ')\n","df_final.head()"]},{"cell_type":"markdown","metadata":{"id":"vDCY4V60fsoF"},"source":["We will define some functions for cleaning of review text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PhNJV3GOqtOp"},"outputs":[],"source":["#Defining function for removing html tags\n","\n","def striphtml(data):\n","    p = re.compile('\u003c.*?\u003e')\n","    return p.sub('',data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VS8eX76Wh-zY"},"outputs":[],"source":["#Defining function for removing punctuation marks\n","\n","def strippunc(data):\n","    p = re.compile(r'[?|!|\\'|\"|#|.|,|)|(|\\|/|~|%|*]')\n","    return p.sub('',data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sEMe0h6CifeX"},"outputs":[],"source":["#Initializing stopwords and SnowballStemmer\n","\n","stop = stopwords.words('english') #All the stopwords in English language\n","snow = SnowballStemmer('english')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"buH3Mj_Yi3dR"},"outputs":[],"source":["#Defining function to convert NLTK tags to WordNet tags\n","\n","# Function: NLTK tags to Wordnet tags\n","\n","def nltk_tag_to_wordnet_tag(nltk_tag):\n","    if nltk_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif nltk_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif nltk_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif nltk_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HyzTaJgVmCoU"},"outputs":[],"source":["#Defining function to tokenize the sentence and return the POS tag for respective tokens\n","\n","def lemmatize_sentence(sentence):\n","    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n","    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) #tuple of (token, wordnet_tag)\n","    lemmatized_sentence = []\n","    for word, tag in wordnet_tagged:\n","#If no available tag, append the token AS IS, else use the tag to lemmatize the token\n","        if tag is None: \n","            lemmatized_sentence.append(snow.stem(word)) \n","        else:\n","            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) #lemmatize the token\n","    return \" \".join(lemmatized_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ozvMMnT0nbKf"},"outputs":[],"source":["#Defining function to carry out preprocessing\n","\n","def preprocess_text(text, stem=False): \n","#transforming text to lower case \n","  text = text.lower()\n","#calling function to remove HTML Tags            \n","  text = striphtml(text)\n","#calling function to remove Punctuation           \n","  text = strippunc(text)           \n","  return lemmatize_sentence(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XT3xx_gjO64H"},"outputs":[],"source":["#Preprocessing the dataset, creating a 'Review' column which will be used for further analysis\n","\n","#Copying to new dataframe \n","df_main = df_final.copy(deep = True)\n","#Creating new column - Review\n","df_main['Review'] = df_main['user_reviews'].map(preprocess_text)\n","#Removing stop words from the new column - Review\n","df_main['Review'] = df_main['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","df_main.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-6VuWoZa6Kvo"},"outputs":[],"source":["#Defining function for plotting common Words in given column\n","\n","def common_wds(column, terms, title_label):\n","  all_words_column = ' '.join([text for text in column])\n","  all_words_column = all_words_column.split()\n","\n","  fr_dist = FreqDist(all_words_column)\n","  words_df = pd.DataFrame({'word':list(fr_dist.keys()), 'count':list(fr_dist.values())})\n","\n","  word_rank = words_df.nlargest(columns=\"count\", n = terms)   # Select Top 20 most frequent words\n","  plt.figure(figsize=(10,5))\n","  ax = sns.scatterplot(data=word_rank, x= \"count\", y = \"word\", color = \"darkred\")\n","  ax.set(ylabel = 'common words')\n","  plt.title(title_label, fontsize = 14)\n","  plt.grid()\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TRvJM_Ch7mBY"},"outputs":[],"source":["#Plotting Common Words in Review column ranked upto 20, using the above defined function\n","\n","common_wds(df_main['Review'],20,'Common Words in Review')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"khX3Ifns8Ddn"},"outputs":[],"source":["#Defining function for plotting least occurring words in given column\n","\n","def rare_wds(column, terms, title_label):\n","  all_words_column = ' '.join([text for text in column])\n","  all_words_column = all_words_column.split()\n","\n","  fr_dist = FreqDist(all_words_column)\n","  words_df = pd.DataFrame({'word':list(fr_dist.keys()), 'count':list(fr_dist.values())})\n","\n","  # selecting top 20 most frequent words\n","  word_rank = words_df.nsmallest(columns=\"count\", n = terms) \n","  plt.figure(figsize=(10,5))\n","  ax = sns.scatterplot(data=word_rank, x= \"count\", y = \"word\", color = \"darkred\")\n","  ax.set(ylabel = 'rare words')\n","  plt.title(title_label, fontsize = 14)\n","  plt.grid()\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FDIaoqR_8LjX"},"outputs":[],"source":["#Plotting Rare Words in Review column ranked upto 20, using function defined above\n","\n","rare_wds(df_main['Review'],20, 'Rare Words in Review')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mz3BtBXe9g-k"},"outputs":[],"source":["#Using wordcloud to view the most frequent words in the Review column\n","\n","from wordcloud import WordCloud, STOPWORDS\n","stopwords = set(STOPWORDS)\n","from matplotlib import pyplot as plt\n","\n","def show_wordcloud(data, title = None):\n","    wordcloud = WordCloud(\n","        background_color='black',\n","        stopwords=stopwords,\n","        max_words=200,\n","        max_font_size=40, \n","        scale=3,\n","        random_state=1 # chosen at random by flipping a coin; it was heads\n",").generate(str(data))\n","\n","    fig = plt.figure(1, figsize=(15, 15))\n","    plt.axis('off')\n","    if title: \n","        fig.suptitle(title, fontsize=20)\n","        fig.subplots_adjust(top=2.3)\n","\n","    plt.imshow(wordcloud)\n","    plt.show()\n","\n","show_wordcloud(df_main['Review'])"]},{"cell_type":"markdown","metadata":{"id":"0AbgGPU1VxS1"},"source":["# **3. Feature extraction**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VxZRNLPdVzqh"},"outputs":[],"source":["#Importing necessary libraries\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n","from sklearn.metrics import mean_squared_error, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2r0XIZQ8WJbe"},"outputs":[],"source":["#Keeping only the relevant columns \n","\n","df_main=df_main[['Review','reviews_rating','user_sentiment']]\n","data=df_main\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LbnX2SplWT0n"},"outputs":[],"source":["#Lets check the new data \n","\n","data.info()"]},{"cell_type":"markdown","metadata":{"id":"F1t-pXu0Wdpq"},"source":["No missing values, data looks good"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5tTVIvaVWgOS"},"outputs":[],"source":["#Saving data for future purpose\n","\n","import pickle as pickle\n","pickle.dump(data, open(\"data.pkl\",\"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fQKMMOZCWom8"},"outputs":[],"source":["#Lets do Feature Extraction using TF-IDF vectorization\n","\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n","tfidf_vectorizer.fit(data['Review'])\n","X = tfidf_vectorizer.transform(data['Review'])\n","y = data['user_sentiment']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4qaUzQGNXEwd"},"outputs":[],"source":["#Saving the vocabulary used in tf-idf vectorizer as features\n","\n","pickle.dump(tfidf_vectorizer.vocabulary_, open(\"features.pkl\",\"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ML142ExiIPx6"},"outputs":[],"source":["#Saving tf-idf vectorizer\n","\n","pickle.dump(tfidf_vectorizer, open(\"tfidf.pkl\", \"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Tdul8jIYXN1C"},"outputs":[],"source":["#Lets split train test data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75)"]},{"cell_type":"markdown","metadata":{"id":"te2ZScj7XbKc"},"source":["Firstly, we will check the class imbalance in the data, handle it (if present) and then proceed with the training "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QVEzmkqGXQl8"},"outputs":[],"source":["#Checking Class Imbalance \n","\n","data.groupby(['user_sentiment']).count()"]},{"cell_type":"markdown","metadata":{"id":"Vvhxg6sZXp1W"},"source":["There is big difference between positive and negative labels, class imbalance is present. We would use SMOTE technique to handle the class imbalance"]},{"cell_type":"markdown","metadata":{"id":"hOaDV4cNYFMA"},"source":["**Handling Class Imbalance**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_h5LybjfX1kX"},"outputs":[],"source":["from collections import Counter\n","from imblearn import over_sampling\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import SMOTE\n","counter = Counter(y_train)\n","print(\"Before\", counter)\n","\n","#oversampling using SMOTE\n","smote = SMOTE()\n","X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n","\n","counter = Counter(y_train_sm)\n","print(\"After\", counter)"]},{"cell_type":"markdown","metadata":{"id":"TJ03hLiVYTjD"},"source":["# **4. Training a text classification model**"]},{"cell_type":"markdown","metadata":{"id":"VvkHkH7hYaBQ"},"source":["We need to build at least three ML models. We then need to analyse the performance of each of these models and choose the best model. At least three out of the following four models need to be built. \n","1. Logistic regression\n","2. Random forest\n","3. XGBoost\n","4. Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6YAiQ9g2ZCs_"},"outputs":[],"source":["#Importing libraries\n","\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn import neighbors\n","from scipy.spatial.distance import cosine\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.pipeline import FeatureUnion\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n","from nltk.stem.porter import PorterStemmer\n","from wordcloud import WordCloud, STOPWORDS\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"brPX6BmYaF2u"},"source":["**Defining metrics for model evaluation**\n","\n","We will now define the metrics based on which models will be evaluated. \n","\n","We will look at the accuracy of the model which will tell us what fraction of prediction is correct.\n","\n","Looking from consumer's point of view, recommending products with negative sentiments will make consumers lose interest in checking the recommended products. It means positive predictive rate should be good. So we will look at the precision of the model. \n","\n","At the same time, missing to recommend products with positive sentiments will cause business loss. So, the sensitivity of the model should also be good. \n","\n","And since we want to look at both preicsion and sensitivity, F1-score will also be useful for us. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v3U5jvqXYKHe"},"outputs":[],"source":["#Defining a function for creating confusion matrix and displaying scores\n","#It will be useful in evaluating all the models\n","\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","\n","def display_score(classifier):\n","    cm = confusion_matrix(y_test, classifier.predict(X_test))\n","    plot_confusion_matrix(classifier, X_test, y_test,include_values=True,values_format='g',cmap=plt.cm.Blues) \n","    p_acc = float(accuracy_score(y_test, classifier.predict(X_test)))  \n","    p_sen = float(format(cm[1][1]/sum(cm[1])))                # sensitivity = true positives/(true positives + false negatives)\n","    p_pre = float(format(cm[1][1]/((cm[1][1])+(cm[0][1]))))   # precision = true positives/(true positives + false positives)\n","    p_f1s = float(format(2*(p_pre * p_sen)/(p_pre + p_sen)))  # F1 = 2*((precision*sensitivity)/(precision+sensitivity))\n","    print(classifier)\n","    print('\\n')\n","    print(f\"Accuracy is {p_acc:.4f}\")\n","    print(f\"Sensitivity is {p_sen:.4f}\")\n","    print(f\"Precision is {p_pre:.4f}\")\n","    print(f\"F1 Score is {p_f1s:.4f}\")\n","    return p_acc, p_sen, p_pre, p_f1s"]},{"cell_type":"markdown","metadata":{"id":"QNZUcXM7Yyyk"},"source":["**Model 1 - Logistic Regression**"]},{"cell_type":"markdown","metadata":{"id":"w3dGpgtbfxLs"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"u2kufsSBYyKv"},"outputs":[],"source":["#Lets try out different learning rates and select the best one\n","\n","for c in [0.01, 0.05, 0.25, 0.5, 1]:\n","    \n","    lr = LogisticRegression(C=c)\n","    lr.fit(X_train_sm, y_train_sm)\n","    cm = confusion_matrix(y_test, lr.predict(X_test))\n","    print('Sensitivity for C = {0} is {1}'.format(c, cm[1][1]/sum(cm[1])))\n","    print('Specificity for C = {0} is {1}'.format(c, cm[0][0]/sum(cm[0])))"]},{"cell_type":"markdown","metadata":{"id":"rgTgBnaYZZAU"},"source":["**Observation**: We will take c=0.05, as it gives the best metrics. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L8VZMyZlZpE2"},"outputs":[],"source":["final_lr = LogisticRegression(C=0.05)\n","final_lr.fit(X_train_sm, y_train_sm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jHldFUlJZsf-"},"outputs":[],"source":["df_lr = display_score(final_lr)\n","df_lr"]},{"cell_type":"markdown","metadata":{"id":"8vRd3Qx-a3lL"},"source":["**Model 2 - Random Forest**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nbFnjmn4asBS"},"outputs":[],"source":["#Fitting a Random Forest classifier without any hyperparameter tuning\n","\n","from sklearn.ensemble import RandomForestClassifier\n","import time\n","\n","from sklearn.model_selection import GridSearchCV\n","rf = RandomForestClassifier()\n","rf.fit(X_train_sm, y_train_sm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3Y7cf-uzbCCK"},"outputs":[],"source":["df_rf = display_score(rf)\n","df_rf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"q0fuSLI9bgUv"},"outputs":[],"source":["#Fitting a Random Forest classifier with various hyperparameters\n","\n","#parameter grid based on the results of random search \n","param_grid = {\n","    'max_depth': [15, 20],\n","    'min_samples_leaf': [100,200],\n","    'min_samples_split': [200,400],\n","    'n_estimators': [100, 300]\n","}\n","\n","\n","final_rf = RandomForestClassifier()\n","\n","# Instantiate the grid search model\n","rf_tuned = GridSearchCV(estimator = final_rf, param_grid = param_grid, scoring='roc_auc', cv = 3, n_jobs = -1,verbose = 1)\n","rf_tuned.fit(X_train_sm, y_train_sm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dbNjdKx9bxQo"},"outputs":[],"source":["#Checking the best hyperparameters\n","\n","print(\"Best AUC-ROC Score on train data: \", rf_tuned.best_score_)\n","print(\"Best hyperparameters: \", rf_tuned.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DEc44xd5dHhW"},"outputs":[],"source":["#Test data performance metrics\n","df_rft = display_score(rf_tuned)\n","df_rft"]},{"cell_type":"markdown","metadata":{"id":"qwJkrYnWeSSq"},"source":["**Model 3 - XGBoost**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vWRNbiDgeX2i"},"outputs":[],"source":["#Fitting a XGBoost classifier without any hyperparameter tuning\n","\n","# importing libraries for XGBoost classifier\n","import xgboost as xgb\n","from xgboost import XGBClassifier\n","\n","final_xgb = XGBClassifier(booster='gbtree')\n","final_xgb.fit(X_train_sm, y_train_sm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mcj__nh4ek5J"},"outputs":[],"source":["#Displaying Confusion matrix Scores\n","\n","df_xgb = display_score(final_xgb)\n","df_xgb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nObWNYLVfN7Z"},"outputs":[],"source":["#Fitting a XGBoost classifier with various custom hyperparameters.\n","\n","param_grid = {'learning_rate': [0.001, 0.01], 'max_depth':[ 5, 10],  'n_estimators':[1, 3]}\n","\n","final_xgb = XGBClassifier(booster='gbtree')\n","\n","# set up GridSearchCV()\n","xgb_tuned = GridSearchCV(estimator = final_xgb, \n","                        param_grid = param_grid, \n","                        scoring= 'roc_auc', \n","                        cv =3, \n","                        verbose = 1,\n","                        return_train_score=True)\n","\n","xgb_tuned.fit(X_train_sm, y_train_sm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ynpD5x-bfbuC"},"outputs":[],"source":["#printing best hyperparameters\n","\n","print(\"Best AUC-ROC Score on train data: \", xgb_tuned.best_score_)\n","print(\"Best hyperparameters: \", xgb_tuned.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"js-CJQe0ffLb"},"outputs":[],"source":["#Displaying Confusion matrix Scores\n","\n","df_xgbt = display_score(xgb_tuned)\n","df_xgbt"]},{"cell_type":"markdown","metadata":{"id":"OjmhzZk_FIUS"},"source":["**Model 4 - Naive Bayes**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jkDldNK2FHVw"},"outputs":[],"source":["#Fitting Naive Bayes Model\n","\n","nb=MultinomialNB()\n","nb.fit(X_train_sm, y_train_sm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tQFfL2ogFWUn"},"outputs":[],"source":["#Test Data Performance Metrics\n","\n","df_nb = display_score(nb)\n","df_nb"]},{"cell_type":"markdown","metadata":{"id":"AV3PV8NKGyjO"},"source":["We are done with model training. Let us now compare the metrics we obtained for each model, based on which we can select our final model for the Sentiment Classification. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eIYwRf1pGwc8"},"outputs":[],"source":["#Displaying metrics in tabular form\n","#Index: 0=Accuracy, 1=Sensitivity, 2=Precision, 3=F1Score\n","\n","results = {('LR'):[df_lr[0],df_lr[1],df_lr[2],df_lr[3]],\n","           ('NB'):[df_nb[0],df_nb[1],df_nb[2],df_nb[3]],\n","           ('XGB'):[df_xgb[0],df_xgb[1],df_xgb[2],df_xgb[3]],\n","           ('XGB Tuned'):[df_xgbt[0],df_xgbt[1],df_xgbt[2],df_xgbt[3]],\n","           ('RF'):[df_rf[0],df_rf[1],df_rf[2],df_rf[3]],\n","           ('RF Tuned'):[df_rft[0],df_rft[1],df_rft[2],df_rft[3]]\n","          }\n","pd.DataFrame(results, index=['Accuracy', 'Sensitivity', 'Precision', 'F1Score'])"]},{"cell_type":"markdown","metadata":{"id":"tUHHKn21S5sw"},"source":["**Model selection**\n","\n","If we look at the accuracy, all models, except XGB-tuned, are comparable.\n","\n","As we had discussed during defining the metrice for evaluation, both precision and sensitivity need to be high. Naive Bayes and Random Forest(without tuning) seem to be the best options in this case. \n"," \n","As F1 score gives equal weight to Precision and Recall, high F1 score means both Precision and Recall are high. NB and RF(without tuning) seem to have best F1-score. \n","\n","Considering all the evaluation points above, **Naive Bayes** and **Random Forest without tuning** seems to be the best choices. \n","I am selecting **Naive Bayes as the final model** here, as the size of Random Forest model pickle file without tuning may be a problem  while uploading on github. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"G3L3cJ1RmJXI"},"outputs":[],"source":["#Saving the final model \n","\n","saved_model = pickle.dump(nb, open('naive_bayes_model.pkl', 'wb'))"]},{"cell_type":"markdown","metadata":{"id":"WVxLUOaZ7_9P"},"source":["# **5. Building a recommendation system**\n","\n","We will build the following types of recommendation systems.\n","\n","1. User-based recommendation system\n","2. Item-based recommendation system\n","\n","Then we will analyse the recommendation systems and select the one that is best suited in this case. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qskwWTqH7_AU"},"outputs":[],"source":["# Importing Libraries\n","\n","from sklearn.metrics.pairwise import pairwise_distances"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XkZYfVuI9nzO"},"outputs":[],"source":["#Lets start with reading the original data file, as we did some data cleaning and preprocessing to the already read file for sentiment analysis.\n","\n","ratings = pd.read_csv(\"sample30.csv\", sep=',')\n","ratings.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lRUnsIzA-E9E"},"outputs":[],"source":["#We will keep only the relevant columns, i.e. id, reviews_rating and username\n","\n","ratings=ratings[['id', 'reviews_rating', 'reviews_username']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RDov7jrh-QYF"},"outputs":[],"source":["#Checking for the null values\n","\n","ratings.info()"]},{"cell_type":"markdown","metadata":{"id":"2nBo1H8gASgk"},"source":["There are 63 records with missing username. We will drop these records. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ROUC7EdR-gPx"},"outputs":[],"source":["#Dropping missing values\n","ratings = ratings[~ratings.reviews_username.isna()]\n","#Renaming the columns for ease to handle\n","ratings.columns=['productId', 'rating', 'user']\n","ratings.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PSKyqMgZE1s5"},"outputs":[],"source":["ratings.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z0WNkH7-EseO"},"outputs":[],"source":["#Splitting the data into train and test datasets\n","\n","train, test = train_test_split(ratings, test_size=0.30, random_state=12)\n","\n","print(train.shape)\n","print(test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MYtq76EuFGjd"},"outputs":[],"source":["#Pivot the train ratings dataset into matrix format in which columns are productId and the rows are username\n","\n","df_pivot = train.pivot_table(\n","    index='user',\n","    columns='productId',\n","    values='rating'\n",").fillna(0)\n","\n","df_pivot.head(3)"]},{"cell_type":"markdown","metadata":{"id":"w8qTyWDjHVnQ"},"source":["**Creating dummy train \u0026 dummy test dataset**\n","These dataset will be used for prediction \n","- Dummy train will be used later for prediction of the products which has not been rated by the user. To ignore the products rated by the user, we will mark it as 0 during prediction. The products not rated by user is marked as 1 for prediction in dummy train dataset. \n","\n","- Dummy test will be used for evaluation. To evaluate, we will only make prediction on the products rated by the user. So, this is marked as 1. This is just opposite of dummy_train."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1AbKxkwYHTj4"},"outputs":[],"source":["#Copying the train dataset into dummy_train\n","dummy_train = train.copy()\n","dummy_train.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Cwh9wvZnO36o"},"outputs":[],"source":["#The products not rated by user is marked as 1 for prediction. \n","\n","dummy_train['rating'] = dummy_train['rating'].apply(lambda x: 0 if x\u003e=1 else 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ttetvEOzPIQE"},"outputs":[],"source":["#Converting the dummy train dataset into matrix format.\n","\n","dummy_train = dummy_train.pivot_table(\n","    index='user',\n","    columns='productId',\n","    values='rating'\n",").fillna(1)\n","\n","\n","dummy_train.head()"]},{"cell_type":"markdown","metadata":{"id":"o2juMUjePq44"},"source":["# **User Based Similarity**"]},{"cell_type":"markdown","metadata":{"id":"zNw71D3ePxde"},"source":["**Cosine Similarity**\n","\n","Cosine Similarity is a measurement that quantifies the similarity between two vectors, which is Rating Vector in this case."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"820cUgYcPwti"},"outputs":[],"source":["df_pivot.index.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"srqXnRp2QUMr"},"outputs":[],"source":["#User Similarity Matrix via pairwise_distance function\n","\n","user_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')\n","user_correlation[np.isnan(user_correlation)] = 0\n","print(user_correlation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fwU5kiThQcJo"},"outputs":[],"source":["user_correlation.shape"]},{"cell_type":"markdown","metadata":{"id":"WhMuH827Qm1A"},"source":["**Adjusted Cosine**\n","\n","Adjusted cosine similarity is a modified version of vector-based similarity where we incorporate the fact that different users have different ratings schemes. In other words, some users might rate items highly in general, and others might give items lower ratings as a preference. To handle this nature from rating given by user , we subtract average ratings for each user from each user's rating for different movies."]},{"cell_type":"markdown","metadata":{"id":"5b7SfzjnRM1A"},"source":["Here, we are not removing the NaN values and calculating the mean only for the movies rated by the user"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7bBMiQLxQrFz"},"outputs":[],"source":["# Create a user-product matrix.\n","\n","df_pivot = train.pivot_table(\n","    index='user',\n","    columns='productId',\n","    values='rating'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oePRmtaQRDoo"},"outputs":[],"source":["#Normalising the rating of the movie for each user around 0 mean\n","\n","mean = np.nanmean(df_pivot, axis=1)\n","df_subtracted = (df_pivot.T-mean).T\n","\n","df_subtracted.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CgeogdEuRVPR"},"outputs":[],"source":["#Creating the User Similarity Matrix using pairwise_distance function\n","\n","user_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')\n","user_correlation[np.isnan(user_correlation)] = 0\n","\n","print(user_correlation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CnjSPIPaRuAI"},"outputs":[],"source":["user_correlation.shape"]},{"cell_type":"markdown","metadata":{"id":"HbDItMfvRwIY"},"source":["**Prediction - User User**\n","\n","Doing the prediction for the users which are positively related with other users, and not the users which are negatively related as we are interested in the users which are more similar to the current users. So, ignoring the correlation for values less than 0."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6z6Ih1vaR6Uv"},"outputs":[],"source":["user_correlation[user_correlation\u003c0]=0\n","user_correlation"]},{"cell_type":"markdown","metadata":{"id":"gqLm5gBkSEcZ"},"source":["Rating predicted by the user (for products rated as well as not rated) is the weighted sum of correlation with the product rating (as present in the rating dataset)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C9oYAfSqSKL4"},"outputs":[],"source":["user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))\n","user_predicted_ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DsPINpv6SR9o"},"outputs":[],"source":["user_predicted_ratings.shape"]},{"cell_type":"markdown","metadata":{"id":"S-xQxQBrSYz7"},"source":["Since we are interested only in the products not rated by the user, we will ignore the products rated by the user by setting it to zero."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ezkgYDuMSX5k"},"outputs":[],"source":["user_final_rating = np.multiply(user_predicted_ratings,dummy_train)\n","user_final_rating.head()"]},{"cell_type":"markdown","metadata":{"id":"j_BYgnKCSmgn"},"source":["**Finding the top 20 recommendation for the user**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"D_ZtwWPYSiyV"},"outputs":[],"source":["#Lets take a random user ID from the given dataset as input\n","\n","user_input='joshua'\n","print(user_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Mjkh5SeVTcZj"},"outputs":[],"source":["#Top 20 recommendations\n","d = user_final_rating.loc[user_input].sort_values(ascending=False)[0:20]\n","d"]},{"cell_type":"markdown","metadata":{"id":"HDNowfSdKWCz"},"source":["**Evaluation - User User**"]},{"cell_type":"markdown","metadata":{"id":"lERZEAowKvQ5"},"source":["Evaluation will be same as for the prediction. The only difference being, we will evaluate for the products already rated by the user insead of predicting it for the products not rated by the user."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pPlhKNgbgnAl"},"outputs":[],"source":["# Find out the common users of test and train dataset.\n","common = test[test.user.isin(train.user)]\n","common.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QYdMCNdAN6PV"},"outputs":[],"source":["common.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"drFBBxA2OBii"},"outputs":[],"source":["#Converting into the user-movie matrix\n","\n","common_user_based_matrix = common.pivot_table(index='user', columns='productId', values='rating')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O5LhQgjmOQwP"},"outputs":[],"source":["#Converting the user_correlation matrix into dataframe\n","\n","user_correlation_df = pd.DataFrame(user_correlation)\n","user_correlation_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K-w3TZrMOjEs"},"outputs":[],"source":["user_correlation_df['user'] = df_subtracted.index\n","user_correlation_df.set_index('user',inplace=True)\n","user_correlation_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5Z-NRYWcO3qd"},"outputs":[],"source":["list_name = common.user.tolist()\n","\n","user_correlation_df.columns = df_subtracted.index.tolist()\n","\n","user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uGFEPrxdPFv5"},"outputs":[],"source":["user_correlation_df_1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JdxOfIAIPLZ3"},"outputs":[],"source":["#Taking transpose of the df_1\n","user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hMtFKyAQPidN"},"outputs":[],"source":["#Taking transpose of df_2 \n","\n","user_correlation_df_3 = user_correlation_df_2.T\n","user_correlation_df_3.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O1eh66HBPq_R"},"outputs":[],"source":["user_correlation_df_3.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-Q-xy1ufP02F"},"outputs":[],"source":["#Taking users which are positively correlated with other users\n","\n","user_correlation_df_3[user_correlation_df_3\u003c0]=0\n","\n","common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))\n","common_user_predicted_ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S3nQCNM1QH7a"},"outputs":[],"source":["#Creating dummy copy to mark the products which are already rated by the user as 1 \n","\n","dummy_test = common.copy()\n","\n","dummy_test['rating'] = dummy_test['rating'].apply(lambda x: 1 if x\u003e=1 else 0)\n","\n","dummy_test = dummy_test.pivot_table(index='user', columns='productId', values='rating').fillna(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zwaUFocUQcSX"},"outputs":[],"source":["dummy_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CvCyxwVBQfZW"},"outputs":[],"source":["#Multiplying predicted_ratings df with dummy_test so we are left with ratings of the products which are already rated by the user, \n","#and others will be set to 0\n","\n","common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)\n","common_user_predicted_ratings.head(2)"]},{"cell_type":"markdown","metadata":{"id":"UvpeN7LeRXoj"},"source":["Calculating the RMSE for only the products rated by user. For RMSE, normalising the rating to (1,5) range."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LzaxJkjfRb4d"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from numpy import *\n","\n","#Making a copy of common_users_predicted_ratings and normalizing the rating to (1,5) range\n","\n","X  = common_user_predicted_ratings.copy() \n","X = X[X\u003e0]\n","\n","scaler = MinMaxScaler(feature_range=(1, 5))\n","print(scaler.fit(X))\n","y = (scaler.transform(X))\n","\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uEWlaSTER1HW"},"outputs":[],"source":["common_ = common.pivot_table(index='user', columns='productId', values='rating')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cSMiIOW4R8B1"},"outputs":[],"source":["#Finding total non-NaN value\n","total_non_nan = np.count_nonzero(~np.isnan(y))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KeKT2jYcSBTD"},"outputs":[],"source":["#Calculating and printing rmse for evaluation\n","\n","rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5\n","print(rmse)"]},{"cell_type":"markdown","metadata":{"id":"6St1vLPISUqo"},"source":["Take a note of rmse here. We will be comparing this with rmse of item based similarity recommendation system"]},{"cell_type":"markdown","metadata":{"id":"OitMEEVOSoVu"},"source":["# **Item Based Similarity**"]},{"cell_type":"markdown","metadata":{"id":"ct6QkYSSTBEC"},"source":["Taking the transpose of the rating matrix to normalize the rating around the mean for different products ID. In the user based similarity, we had taken mean for each user instead of each products. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"G3ed_E3QSnlG"},"outputs":[],"source":["df_pivot = train.pivot_table(\n","    index='user',\n","    columns='productId',\n","    values='rating'\n",").T\n","\n","df_pivot.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5l7AQuk6TNMH"},"outputs":[],"source":["#Normalising the Product rating for each product for using the Adujsted Cosine\n","\n","mean = np.nanmean(df_pivot, axis=1)\n","df_subtracted = (df_pivot.T-mean).T\n","\n","df_subtracted.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kCO7gNy1Tc3W"},"outputs":[],"source":["#Finding the Cosine Similarity using pairwise distances approach\n","\n","# Item Similarity Matrix\n","item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')\n","item_correlation[np.isnan(item_correlation)] = 0\n","print(item_correlation)\n","print(item_correlation.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1IC6Gmc8TtHb"},"outputs":[],"source":["#Filtering for positive correlation - only for which the value is greater than 0\n","\n","item_correlation[item_correlation\u003c0]=0\n","item_correlation"]},{"cell_type":"markdown","metadata":{"id":"KvIVugHRT3Sr"},"source":["**Prediction - Item Item**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GGtvmSiYT6D_"},"outputs":[],"source":["#Predicting the rating based on item similarity\n","\n","item_predicted_ratings = np.dot((df_pivot.fillna(0).T),item_correlation)\n","item_predicted_ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iWE5aV6EUBnX"},"outputs":[],"source":["item_predicted_ratings.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"umhw0N5FUJrw"},"outputs":[],"source":["dummy_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GGcLkUJ2URGz"},"outputs":[],"source":["#Filtering the rating only for the products not rated by the user for recommendation, by multiplying with dummy_train\n","\n","item_final_rating = np.multiply(item_predicted_ratings,dummy_train)\n","item_final_rating.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e4RVvI09f3B8"},"outputs":[],"source":["# Take a random user ID from dataset as input\n","\n","user_input='zipperdoo'\n","print(user_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Lb_6V88egUCE"},"outputs":[],"source":["#Recommending the Top 20 products to the user.\n","d = item_final_rating.loc[user_input].sort_values(ascending=False)[0:20]\n","d"]},{"cell_type":"markdown","metadata":{"id":"hB7SgK31gdJm"},"source":["**Evaluation - Item Item**"]},{"cell_type":"markdown","metadata":{"id":"5E61T41eg0ur"},"source":["Evaluation will be same as for the prediction. The only difference being, we will evaluate for the products already rated by the user insead of predicting it for the products not rated by the user."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qVHospogg0ut"},"outputs":[],"source":["# Find out the common usersproducts of test and train dataset.\n","common =  test[test.productId.isin(train.productId)]\n","common.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6nP52SFrg0uv"},"outputs":[],"source":["common.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OJ-WIYKHg0uw"},"outputs":[],"source":["#Converting into the user-product matrix, taking transpose\n","\n","common_item_based_matrix = common.pivot_table(index='user', columns='productId', values='rating').T\n","common_item_based_matrix.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RBvCdvT3g0uy"},"outputs":[],"source":["#Converting the user_correlation matrix into dataframe\n","\n","item_correlation_df = pd.DataFrame(item_correlation)\n","item_correlation_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2qVRVg95g0uz"},"outputs":[],"source":["item_correlation_df['productId'] = df_subtracted.index\n","item_correlation_df.set_index('productId',inplace=True)\n","item_correlation_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y7lT09Hpg0u0"},"outputs":[],"source":["list_name = common.productId.tolist()\n","\n","item_correlation_df.columns = df_subtracted.index.tolist()\n","\n","item_correlation_df_1 =  item_correlation_df[item_correlation_df.index.isin(list_name)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mFANk_NDg0u1"},"outputs":[],"source":["item_correlation_df_1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1BBlttzdg0u2"},"outputs":[],"source":["item_correlation_df_2 = item_correlation_df_1.T[item_correlation_df_1.T.index.isin(list_name)]\n","\n","item_correlation_df_3 = item_correlation_df_2.T"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rxTGo918g0u3"},"outputs":[],"source":["item_correlation_df_3.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wfrq2APgg0u4"},"outputs":[],"source":["item_correlation_df_3.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5T4tnGDvg0u5"},"outputs":[],"source":["#Taking item which are positively correlated with other items\n","\n","item_correlation_df_3[item_correlation_df_3\u003c0]=0\n","\n","common_item_predicted_ratings = np.dot(item_correlation_df_3, common_item_based_matrix.fillna(0))\n","common_item_predicted_ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J0hMka5mjBua"},"outputs":[],"source":["common_item_predicted_ratings.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IsYxNwnig0u7"},"outputs":[],"source":["#Creating dummy copy to mark the products which are already rated by the user as 1 \n","\n","dummy_test = common.copy()\n","\n","dummy_test['rating'] = dummy_test['rating'].apply(lambda x: 1 if x\u003e=1 else 0)\n","\n","dummy_test = dummy_test.pivot_table(index='user', columns='productId', values='rating').T.fillna(0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CpT56HO4g0u8"},"outputs":[],"source":["dummy_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uAq0lZ-Ug0u9"},"outputs":[],"source":["#Multiplying predicted_ratings df with dummy_test so we are left with ratings of the products which are already rated by the user, \n","#and others will be set to 0\n","\n","common_item_predicted_ratings = np.multiply(common_item_predicted_ratings,dummy_test)\n","common_item_predicted_ratings.head(2)"]},{"cell_type":"markdown","metadata":{"id":"qQkHpLDsg0vC"},"source":["Calculating the RMSE for only the products rated by user. For RMSE, normalising the rating to (1,5) range."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2zf0eHBzg0vD"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from numpy import *\n","\n","#Making a copy of common_item_predicted_ratings and normalizing the rating to (1,5) range\n","\n","X  = common_item_predicted_ratings.copy() \n","X = X[X\u003e0]\n","\n","scaler = MinMaxScaler(feature_range=(1, 5))\n","print(scaler.fit(X))\n","y = (scaler.transform(X))\n","\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"piRS7IWlg0vE"},"outputs":[],"source":["#Finding total non-NaN value\n","total_non_nan = np.count_nonzero(~np.isnan(y))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"I5YLh-GNkc1F"},"outputs":[],"source":["common_ = common.pivot_table(index='user', columns='productId', values='rating').T"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5b1pPIm5g0vF"},"outputs":[],"source":["#Calculating and printing rmse for evaluation\n","\n","rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5\n","print(rmse)"]},{"cell_type":"markdown","metadata":{"id":"zeD0bj5bg0vF"},"source":["**Selecting recommendation system**\n","\n","rmse value for user similarity based system = 2.589725958923943\n","\n","rmse value for item similarity based system = 3.5462471410112615\n","\n","Based on rmse value, we select user similarity based recommendation system, as its rmse is smaller"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1dpCTulQlXvB"},"outputs":[],"source":["# saving the model\n","pickle.dump(user_final_rating.astype('float32'), open('user_final_rating.pkl', 'wb'))"]},{"cell_type":"markdown","metadata":{"id":"DWRVVV5K2MXA"},"source":["-- End of the notebook -- "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOL6pt1js1SkLhwupAHdxxI","collapsed_sections":[],"name":"Copy of Copy of Sentiment_Analysis.ipynb","provenance":[{"file_id":"15mMTQnPyLjO1oPcJ-Y3sp1uUriPuTGAp","timestamp":1639075471741},{"file_id":"1oMv4kF3irnmQde_bJDGFmHjbA5llicLb","timestamp":1639060330554},{"file_id":"1s49nYFrp_USPX8HkuW5WTuoTYy4cItij","timestamp":1638898615200}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}